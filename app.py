import streamlit as st
import torch
from PIL import Image
import io
import os
from transformers import CLIPProcessor, CLIPTokenizer
from models import ViTImageCaptioningModel
from torchvision import transforms
import numpy as np
import time

# Set page config
st.set_page_config(
    page_title="Image Captioning",
    page_icon="üñºÔ∏è",
    layout="centered"
)

@st.cache_resource
def load_model():
    """Load the image captioning model and necessary processors"""
    # Set device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # Load model
    clip_model_name = "openai/clip-vit-base-patch32"
    model = ViTImageCaptioningModel(clip_model_name=clip_model_name).to(device)
    
    # Load the model weights - adjust path as needed
    model_path = os.environ.get("MODEL_PATH", "models/best_image_caption_model.pt")
    if not os.path.exists(model_path):
        # In HuggingFace spaces, models might be in a different location
        model_path = os.environ.get("MODEL_PATH", "/models/best_image_caption_model.pt") 
        
    # Try to download from HuggingFace if not found locally
    if not os.path.exists(model_path):
        try:
            from huggingface_hub import hf_hub_download
            repo_id = os.environ.get("HF_REPO_ID", "YOUR_HUGGINGFACE_USERNAME/image-captioning-model")
            model_path = hf_hub_download(repo_id=repo_id, filename="best_image_caption_model.pt")
        except:
            st.error("‚ö†Ô∏è Model not found! Please check the model path or set the HF_REPO_ID environment variable.")
    
    # Load model weights
    try:
        model.load_state_dict(torch.load(model_path, map_location=device))
        model.eval()
    except Exception as e:
        st.error(f"‚ö†Ô∏è Error loading model: {str(e)}")
        
    # Load tokenizer and processor
    processor = CLIPProcessor.from_pretrained(clip_model_name)
    tokenizer = CLIPTokenizer.from_pretrained(clip_model_name)
    
    return model, processor, tokenizer, device

def process_image(image, processor):
    """Process an uploaded image for the model"""
    transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor()
    ])
    
    # Apply transformation to image
    image_tensor = transform(image)
    
    return image_tensor

def generate_caption(model, image_tensor, processor, tokenizer, device, max_length=20):
    """Generate a caption for the processed image"""
    # Move image to device
    image_tensor = image_tensor.unsqueeze(0).to(device)  # Add batch dimension
    
    # Process image with CLIP processor
    inputs = processor(
        images=image_tensor,
        return_tensors="pt",
        do_rescale=False
    )
    
    # Move inputs to device
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    # Get image features
    tokenized_images = inputs["pixel_values"]
    
    # Start with BOS token
    input_ids = torch.tensor([[tokenizer.bos_token_id]]).to(device)
    
    # Generate tokens auto-regressively
    with torch.no_grad():
        for _ in range(max_length):
            # Forward pass
            output_logits = model(tokenized_images, input_ids)
            
            # Get the prediction for the last token
            next_token_logits = output_logits[0, -1, :]
            
            # Get most likely token
            next_token_id = torch.argmax(next_token_logits).unsqueeze(0).unsqueeze(0)
            
            # Add to input sequence
            input_ids = torch.cat([input_ids, next_token_id], dim=1)
            
            # Stop if EOS token is generated
            if next_token_id.item() == tokenizer.eos_token_id:
                break
    
    # Decode the caption
    caption = tokenizer.decode(input_ids[0], skip_special_tokens=True)
    return caption

def main():
    st.title("üì∏ Image Captioning App")
    st.write("Upload an image and get a caption generated by our AI model!")
    
    # Load model (cached)
    with st.spinner("Loading model..."):
        model, processor, tokenizer, device = load_model()
    
    # File uploader
    uploaded_file = st.file_uploader("Choose an image...", type=["jpg", "jpeg", "png"])
    
    # Camera input option
    camera_image = st.camera_input("Or take a photo")
    
    # Process the uploaded image or camera image
    if uploaded_file is not None or camera_image is not None:
        input_file = uploaded_file if uploaded_file is not None else camera_image
        
        # Convert bytes to PIL Image
        image = Image.open(input_file).convert("RGB")
        
        # Display the image
        st.image(image, caption="Uploaded Image", use_column_width=True)
        
        # Process image when user clicks the button
        if st.button("Generate Caption"):
            with st.spinner("Generating caption..."):
                # Process the image
                processed_image = process_image(image, processor)
                
                # Time the caption generation
                start_time = time.time()
                caption = generate_caption(model, processed_image, processor, tokenizer, device)
                end_time = time.time()
                
                # Display the results
                st.success(f"Caption generated in {end_time - start_time:.2f} seconds")
                st.subheader("Generated Caption:")
                st.write(f"**{caption}**")
                
                # Add some information about the model
                with st.expander("About this model"):
                    st.write("""
                    This image captioning model is based on CLIP's vision encoder and a custom decoder,
                    trained on the Flickr30k dataset. It combines computer vision and natural language processing
                    to generate descriptive captions for images.
                    """)

if __name__ == "__main__":
    main()